{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "toc_visible": true,
      "mount_file_id": "1NJju9no-Smq1tQAAggunaWiEr7-Z_nSR",
      "authorship_tag": "ABX9TyOcjkBt9MWxnwDzyzQz4+x9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/le-pigeon/fantastic-winner/blob/main/SAR_DeepSpeck.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Colab test"
      ],
      "metadata": {
        "id": "pENBQJe15bq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# empty for debug/admin"
      ],
      "metadata": {
        "id": "84Yw17iGGPxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This script trains a deep unrolling network for SAR despeckling.\n",
        "# Steps:\n",
        "# 0. Pay for premium GPU :)\n",
        "# 1. Set up dataset (simulated speckle noise on optical images)\n",
        "# 2. Define model\n",
        "# 3. Train with Charbonnier + Total Variation loss\n",
        "# 4. Test on real SAR images/test noisy images"
      ],
      "metadata": {
        "id": "vZ-Z10YNLw4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "import os\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "import gc\n",
        "from tqdm import tqdm  # For progress bar\n",
        "import torch.cuda.memory as cuda_mem\n",
        "\n"
      ],
      "metadata": {
        "id": "bgwbh5Cr3sjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you wanna clear cache.. idk sometimes still high RAM :[\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "MsjzHsI-EWDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This block loads and unzip learning dataset from G Drive\n",
        "# Allow all permissions\n",
        "# You should see /content/drive folder\n",
        "\n",
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# drive.mount('/content/drive')  # Connect to your Google Drive\n",
        "\n",
        "# Define paths for zip files\n",
        "zip_path = \"/content/drive/MyDrive/SAR_Project/Dataset/SAR_paired.zip\"\n",
        "extract_path = \"/content/test_data/SAR_Dataset\"  # Where to extract\n",
        "\n",
        "# Make folder to put your own test real SAR images\n",
        "folder_name = \"/content/imported_images\"\n",
        "\n",
        "# Create if doesn't exist\n",
        "os.makedirs(folder_name, exist_ok=True)\n",
        "\n",
        "# Step 3: Extract ZIP\n",
        "print(\"Extracting dataset... this may take a moment!\")\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Extraction complete!\")\n",
        "\n",
        "# Make model folder if you want to load your own model\n",
        "os.makedirs(\"/content/model\", exist_ok=True)  # Make a folder exist\n",
        "\n",
        "# Select CUDA device (GPU is recommended)\n",
        "# Automatically select CPU if no GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "id": "Zbp1kBQN6Djw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load training data and validation data"
      ],
      "metadata": {
        "id": "H2fGli2U_QEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "############################## PAIRED DATASET #################################\n",
        "###############################################################################\n",
        "\n",
        "# Load training\n",
        "train_noisy_tensors = []\n",
        "train_clean_tensors = []\n",
        "\n",
        "# Path to folders\n",
        "noisy_folder = \"/content/test_data/SAR_Dataset/SAR despeckling filters dataset/Main folder/Noisy\"\n",
        "clean_folder = \"/content/test_data/SAR_Dataset/SAR despeckling filters dataset/Main folder/GTruth\"\n",
        "\n",
        "# Remove too black images\n",
        "def is_black_image(filepath, brightness_threshold=0.1, black_ratio_threshold=0.8):\n",
        "    \"\"\"\n",
        "    Checks if an image is mostly black.\n",
        "\n",
        "    Parameters:\n",
        "        filepath (str): Path to the image file.\n",
        "        brightness_threshold (float): Pixel intensity below which a pixel is considered black (0-1 scale).\n",
        "        black_ratio_threshold (float): Percentage of pixels that must be black for removal.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the image is mostly black, False otherwise.\n",
        "    \"\"\"\n",
        "    img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
        "    if img is None:\n",
        "        return True  # Treat unreadable images as black\n",
        "\n",
        "    img = img / 255.0  # Normalize to [0,1]\n",
        "    black_pixels = np.sum(img < brightness_threshold)\n",
        "    total_pixels = img.size\n",
        "    black_ratio = black_pixels / total_pixels\n",
        "\n",
        "    return black_ratio > black_ratio_threshold  # Remove if too much is black\n",
        "\n",
        "def is_mostly_black_edge_case(filepath, brightness_threshold=0.1, black_ratio_threshold=0.9, center_crop_ratio=0.5):\n",
        "    \"\"\"\n",
        "    Detects if an image is mostly black, even if there are small bright stripes at the edges.\n",
        "\n",
        "    Args:\n",
        "        filepath: Path to image.\n",
        "        brightness_threshold: Pixel value below which it's considered black (0-1 scale).\n",
        "        black_ratio_threshold: Proportion of black pixels to consider it \"too black\".\n",
        "        center_crop_ratio: Proportion of image to focus on the center (e.g., 0.5 = center 50%).\n",
        "\n",
        "    Returns:\n",
        "        True if mostly black, False otherwise.\n",
        "    \"\"\"\n",
        "    img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
        "    if img is None:\n",
        "        return True\n",
        "\n",
        "    img = img / 255.0\n",
        "    h, w = img.shape\n",
        "\n",
        "    # Focus on central region\n",
        "    ch, cw = int(h * center_crop_ratio), int(w * center_crop_ratio)\n",
        "    y1, x1 = (h - ch) // 2, (w - cw) // 2\n",
        "    center_crop = img[y1:y1+ch, x1:x1+cw]\n",
        "\n",
        "    black_pixels = np.sum(center_crop < brightness_threshold)\n",
        "    total_pixels = center_crop.size\n",
        "    black_ratio = black_pixels / total_pixels\n",
        "\n",
        "    return black_ratio > black_ratio_threshold\n",
        "\n",
        "#Remove pair from training if clean image is too blurry\n",
        "def is_blurry(image, threshold=100):\n",
        "    \"\"\"Detect if an image is blurry using Laplacian variance.\"\"\"\n",
        "    gray = image if len(image.shape) == 2 else cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    laplacian = cv2.Laplacian(gray, cv2.CV_64F)\n",
        "    variance = laplacian.var()\n",
        "    return variance < threshold  # Low variance = blurry image\n",
        "\n",
        "# Load all image paths\n",
        "noisy_paths = sorted(glob.glob(os.path.join(noisy_folder, \"*.tiff\")))\n",
        "clean_paths = sorted(glob.glob(os.path.join(clean_folder, \"*.tiff\")))\n",
        "\n",
        "# Load the images to CPU RAM\n",
        "for noisy_path, clean_path in zip(noisy_paths, clean_paths):\n",
        "    #if is_black_image(noisy_path) or is_black_image(clean_path):\n",
        "    # if is_mostly_black_edge_case(noisy_path) or is_mostly_black_edge_case(clean_path):\n",
        "    #       continue  # Skip black images\n",
        "\n",
        "    noisy = cv2.imread(noisy_path, cv2.IMREAD_GRAYSCALE)\n",
        "    clean = cv2.imread(clean_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # if is_blurry(clean):  # Only check CLEAN SAR image\n",
        "    #     # print(f\"Skipping blurry clean image: {clean_path}\")\n",
        "    #     continue  # Skip both noisy and clean images if clean is blurry\n",
        "\n",
        "    # Resize to save memory\n",
        "    noisy = cv2.resize(noisy, (256, 256), interpolation=cv2.INTER_AREA)\n",
        "    clean = cv2.resize(clean, (256, 256), interpolation=cv2.INTER_AREA)\n",
        "    noisy = noisy / 255.0\n",
        "    clean = clean / 255.0\n",
        "\n",
        "    # Store as torch tensors (but stay in CPU RAM)\n",
        "    train_noisy_tensors.append(torch.tensor(noisy).unsqueeze(0))  # (1, H, W)\n",
        "    train_clean_tensors.append(torch.tensor(clean).unsqueeze(0))  # (1, H, W)\n",
        "\n",
        "print(f\"Loaded {len(train_clean_tensors)} noisy-clean SAR image pairs to CPU RAM!\")\n",
        "\n",
        "\n",
        "\n",
        "class PairedSARDataset(Dataset):\n",
        "    def __init__(self, noisy_images, clean_images):\n",
        "        self.noisy_images = noisy_images\n",
        "        self.clean_images = clean_images\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        noisy = self.noisy_images[index]\n",
        "        clean = self.clean_images[index]\n",
        "\n",
        "        # Move to GPU only when accessed\n",
        "        return noisy.float(), clean.float()\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.noisy_images)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZsllO7aO3Nja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This loads training data\n",
        "\n",
        "SEED = 42  # Set a fixed seed for reproducibility on random\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "torch.backends.cudnn.benchmark = True  # Enable for speed & efficiency\n",
        "\n",
        "# Ensure reproducibility across NumPy & PyTorch\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# Create Paired Dataset\n",
        "train_dataset = PairedSARDataset(train_noisy_tensors, train_clean_tensors)\n",
        "subset_indices = np.random.choice(len(train_dataset), 800, replace=True)\n",
        "subset = Subset(train_dataset, subset_indices)\n",
        "\n",
        "# To load all dataset, uncomment if needed\n",
        "# train_loader = DataLoader(train_dataset,\n",
        "#                           batch_size=4,\n",
        "#                           shuffle=True,\n",
        "#                           num_workers=2,\n",
        "#                           pin_memory=False)  # Pin to CPU RAM\n",
        "\n",
        "# To load only a subset (more GPU efficient)\n",
        "train_loader = DataLoader(subset,\n",
        "                          batch_size=16,\n",
        "                          shuffle=True,\n",
        "                          num_workers=8,\n",
        "                          pin_memory=False)  # Pin to CPU RAM\n",
        "\n",
        "# print(f\"Ready to train on {len(train_dataset)} images!\") # uncomment if using all images\n",
        "print(f\"Ready to train on {len(subset)} images!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "yVkZZINK40s_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This loads validation dataset\n",
        "val_noisy_tensors = []\n",
        "val_clean_tensors = []\n",
        "\n",
        "# Load validationn data set\n",
        "val_noisy_dir = \"/content/test_data/SAR_Dataset/SAR despeckling filters dataset/Main folder/Noisy_val\"\n",
        "val_clean_dir = \"/content/test_data/SAR_Dataset/SAR despeckling filters dataset/Main folder/GTruth_val\"\n",
        "\n",
        "# List of filenames to exclude\n",
        "exclude_files = {'5120_0.tiff', '5632_0.tiff'}\n",
        "\n",
        "val_noisy_files = sorted(\n",
        "    [f for f in glob.glob(os.path.join(val_noisy_dir, \"*.tiff\")) if os.path.basename(f) not in exclude_files]\n",
        ")\n",
        "\n",
        "val_clean_files = sorted(\n",
        "    [f for f in glob.glob(os.path.join(val_clean_dir, \"*.tiff\")) if os.path.basename(f) not in exclude_files]\n",
        ")\n",
        "\n",
        "\n",
        "# Sanity check\n",
        "assert len(val_noisy_files) == len(val_clean_files), \"Mismatch in validation pair count!\"\n",
        "\n",
        "for noisy_path, clean_path in zip(val_noisy_files, val_clean_files):\n",
        "    noisy = cv2.imread(noisy_path, cv2.IMREAD_GRAYSCALE)\n",
        "    clean = cv2.imread(clean_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # Normalize to [0,1]\n",
        "    noisy = torch.tensor(noisy / 255.0, dtype=torch.float32).unsqueeze(0)\n",
        "    clean = torch.tensor(clean / 255.0, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "    noisy = noisy / 255.0\n",
        "    clean = clean / 255.0\n",
        "\n",
        "    val_noisy_tensors.append(noisy)\n",
        "    val_clean_tensors.append(clean)\n",
        "\n",
        "# Pass the tensors\n",
        "val_dataset = PairedSARDataset(val_noisy_tensors, val_clean_tensors)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=8)\n",
        "\n",
        "print(f\"Ready to validate on {len(val_dataset)} images!\")"
      ],
      "metadata": {
        "id": "zgInyqA-bFq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Peek at training images OwO\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "\n",
        "# Function to show images\n",
        "def show_random_images(noisy_images, clean_images, num_samples=15):\n",
        "    \"\"\"Display random noisy and clean SAR images.\"\"\"\n",
        "    indices = random.sample(range(len(noisy_images)), num_samples)\n",
        "\n",
        "    for idx in indices:\n",
        "        noisy = cv2.imread(noisy_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
        "        clean = cv2.imread(clean_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        noisy = noisy / 255.0\n",
        "        clean = clean / 255.0\n",
        "\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.imshow(noisy, cmap='gray')\n",
        "        plt.title(\"Noisy SAR Image\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.imshow(clean, cmap='gray')\n",
        "        plt.title(\"Clean SAR Image\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "# Show 10 random images from the dataset\n",
        "show_random_images(train_noisy_tensors, train_clean_tensors)"
      ],
      "metadata": {
        "id": "XrC0EYaEGaSl",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load autoencoder and model"
      ],
      "metadata": {
        "id": "cORdLl_W_EBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResUNet_512(nn.Module):\n",
        "    def __init__(self, channels=1):\n",
        "        super(ResUNet_512, self).__init__()\n",
        "        # Encoder Path with BatchNorm\n",
        "        self.enc1 = self.conv_block(channels, 64)\n",
        "        self.enc2 = self.conv_block(64, 128)\n",
        "        self.enc3 = self.conv_block(128, 256)\n",
        "        self.enc4 = self.conv_block(256, 256)\n",
        "\n",
        "        # Decoder Path with optimized skip connections\n",
        "        self.dec4 = self.upconv_block(256, 256)\n",
        "        self.dec3 = self.upconv_block(256, 128)\n",
        "        self.dec2 = self.upconv_block(128, 64)\n",
        "        self.dec1 = self.conv_block(64, 32)\n",
        "\n",
        "        # Final Output Layer\n",
        "        self.final_conv = nn.Conv2d(32, 1, kernel_size=3, padding=1)\n",
        "\n",
        "    def conv_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.1)\n",
        "        )\n",
        "\n",
        "    def upconv_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3,\n",
        "                               stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.1)\n",
        "        )\n",
        "\n",
        "    def crop(self, enc_feat, dec_feat):\n",
        "        \"\"\"Crop encoder features to match decoder features spatially.\"\"\"\n",
        "        _, _, h, w = dec_feat.size()\n",
        "        return enc_feat[:, :, :h, :w]\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(F.max_pool2d(e1, 2))\n",
        "        e3 = self.enc3(F.max_pool2d(e2, 2))\n",
        "        e4 = self.enc4(F.max_pool2d(e3, 2))\n",
        "\n",
        "        # Decoder with optimized skip connections\n",
        "        d4_temp = self.dec4(e4)\n",
        "        d4 = d4_temp + self.crop(e3, d4_temp)\n",
        "\n",
        "        d3_temp = self.dec3(d4)\n",
        "        d3 = d3_temp + self.crop(e2, d3_temp)\n",
        "\n",
        "        d2_temp = self.dec2(d3)\n",
        "        d2 = d2_temp + self.crop(e1, d2_temp)\n",
        "\n",
        "        d1 = self.dec1(d2)\n",
        "\n",
        "        # Residual subtraction (with scaling)\n",
        "        out = self.final_conv(d1)\n",
        "        return F.relu(out)\n"
      ],
      "metadata": {
        "id": "xp09wmXS1dbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Debug to check resUnet output (was broken a few times before hand)\n",
        "test_input = torch.randn(1, 1, 512, 512)  # Simulated SAR patch\n",
        "model = ResUNet_512()\n",
        "output = model(test_input)\n",
        "\n",
        "print(\"ResUNet Output Shape:\", output.shape)  # Should match input e.g. (1, 1, 512, 512)\n",
        "\n"
      ],
      "metadata": {
        "id": "81B6SFqWMMtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uCuFx0C5Z3I"
      },
      "outputs": [],
      "source": [
        "# ========== SAR DeepSpeck model ==========\n",
        "\n",
        "class SAR_DeepSpeck(nn.Module):\n",
        "    def __init__(self, num_layers=8):\n",
        "        super(SAR_DeepSpeck, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.resunet = ResUNet_512()\n",
        "\n",
        "        self.gradient_steps = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(1, 8, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(8),\n",
        "                nn.ReLU(),\n",
        "                nn.Conv2d(8, 1, kernel_size=3, padding=1)\n",
        "            ) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Make δ and η trainable parameters\n",
        "        self.delta = nn.Parameter(torch.tensor(0.01, dtype=torch.float32))  # Init to 0.01\n",
        "        self.eta = nn.Parameter(torch.tensor(1.0, dtype=torch.float32))    # Init to 1.0\n",
        "\n",
        "    def forward(self, x):\n",
        "        v = x  # Initial guess\n",
        "        for i in range(self.num_layers):\n",
        "            noise_est = self.resunet(v)     # Estimate noise at current step\n",
        "            x = x - self.delta * (self.eta * self.gradient_steps[i](noise_est))  # Update x\n",
        "            v = torch.relu(x) # Clamp vlaues to avoid negative value\n",
        "\n",
        "        return x - self.resunet(v)\n",
        "        # return x + v\n",
        "\n",
        "\n",
        "# ========== Loss Function & Optimizer ==========\n",
        "def charbonnier_loss(x, y, epsilon=1e-5):  #  Increase epsilon to prevent division issues\n",
        "    diff = x - y\n",
        "    return torch.mean(torch.sqrt(diff ** 2 + epsilon))\n",
        "\n",
        "def tv_loss(img):\n",
        "    h_variance = torch.mean(torch.abs(img[:, :, 1:, :] - img[:, :, :-1, :]))\n",
        "    w_variance = torch.mean(torch.abs(img[:, :, :, 1:] - img[:, :, :, :-1]))\n",
        "    return h_variance + w_variance\n",
        "\n",
        "\n",
        "def laplacian_loss(img):\n",
        "    img = F.pad(img, (1, 1, 1, 1), mode='reflect')  # Avoid size mismatch\n",
        "    laplacian_kernel = torch.tensor([[0, 1, 0],\n",
        "                                 [1, -4, 1],\n",
        "                                 [0, 1, 0]], dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
        "    laplacian_img = F.conv2d(img, laplacian_kernel, padding=0)\n",
        "    return torch.mean(torch.abs(laplacian_img))  # L1 loss on Laplacian\n",
        "\n",
        "\n",
        "# To check validation loss but..uh doens't really workas intended it keeps going up :( )\n",
        "\n",
        "def gaussian_kernel(window_size=11, sigma=1.5, channels=1):\n",
        "    x = torch.arange(window_size).float() - window_size // 2\n",
        "    gauss = torch.exp(-x**2 / (2 * sigma**2))\n",
        "    gauss = gauss / gauss.sum()\n",
        "    kernel = gauss[:, None] @ gauss[None, :]\n",
        "    kernel = kernel.expand(channels, 1, window_size, window_size)\n",
        "    return kernel\n",
        "\n",
        "\n",
        "def ssim_torch(img1, img2, window_size=11, window=None, C1=0.01**2, C2=0.03**2):\n",
        "    \"\"\"Simplified SSIM for grayscale images.\"\"\"\n",
        "    assert img1.size() == img2.size(), \"Input images must have the same size\"\n",
        "    B, C, H, W = img1.shape\n",
        "\n",
        "    if window is None:\n",
        "        window = gaussian_kernel(window_size=window_size, sigma=1.5, channels=C).to(img1.device)\n",
        "\n",
        "    mu1 = F.conv2d(img1, window, padding=window_size//2, groups=C)\n",
        "    mu2 = F.conv2d(img2, window, padding=window_size//2, groups=C)\n",
        "\n",
        "    mu1_sq = mu1.pow(2)\n",
        "    mu2_sq = mu2.pow(2)\n",
        "    mu1_mu2 = mu1 * mu2\n",
        "\n",
        "    sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size//2, groups=C) - mu1_sq\n",
        "    sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size//2, groups=C) - mu2_sq\n",
        "    sigma12 = F.conv2d(img1 * img2, window, padding=window_size//2, groups=C) - mu1_mu2\n",
        "\n",
        "    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / \\\n",
        "               ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n",
        "    return ssim_map.mean()\n",
        "\n",
        "# ========== Early stopping ==========\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', min_loss=None):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.delta = delta\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = float('inf')\n",
        "        self.path = path\n",
        "        self.min_loss = min_loss  # User-defined absolute threshold\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        # Early exit if minimum threshold is reached\n",
        "        if self.min_loss is not None and val_loss < self.min_loss:\n",
        "            if self.verbose:\n",
        "                print(f\"Validation loss {val_loss:.6f} < threshold {self.min_loss:.6f} → Early stopping now.\")\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.early_stop = True\n",
        "            return\n",
        "\n",
        "        score = -val_loss\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f\"No improvement. EarlyStopping counter: {self.counter}/{self.patience}\")\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        if self.verbose:\n",
        "            print(f\"Saving model ... (val_loss: {val_loss:.6f})\")\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training loop"
      ],
      "metadata": {
        "id": "DmTQdlWA-9ef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =================== Training loop ===========================\n",
        "\n",
        "from torch.amp import autocast, GradScaler\n",
        "scaler = GradScaler()\n",
        "device_type = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "early_stopping = EarlyStopping(patience=5,\n",
        "                               verbose=True,\n",
        "                               delta=0.00001,\n",
        "                               path=\"/content/model/SAR_DeepSpeck.pth\",\n",
        "                               min_loss=0.005\n",
        "                               )\n",
        "\n",
        "# Clear GPU mem\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "model = SAR_DeepSpeck().to(device)  # Move model to CPU or GPU\n",
        "\n",
        "num_epochs = 10  # Set the number of epochs (14 is the magic number?)\n",
        "\n",
        "# Define weights\n",
        "w1 = torch.tensor(0.9)    # Charbonnier\n",
        "# w2 = torch.tensor(0.1)  # Gradient Loss not used\n",
        "w3 = torch.tensor(0.05) # Lap loss\n",
        "w4 = torch.tensor(0.08) # tv loss\n",
        "\n",
        "# Add parameters to optimizer\n",
        "# optimizer = torch.optim.Adam([w1, w2, w3] + list(model.parameters()), lr=1e-5) # for learnable\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=20e-6, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.75, patience=3)\n",
        "\n",
        "\n",
        "# ========= Training Loop ==========\n",
        "torch.set_default_tensor_type('torch.FloatTensor')  # Ensure float32 tensors\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    torch.cuda.empty_cache()  # Free up unused GPU memory\n",
        "\n",
        "    total_loss = 0.0\n",
        "\n",
        "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for batch_idx, (noisy, clean) in progress_bar:\n",
        "        noisy, clean = noisy.float(), clean.float()\n",
        "        noisy, clean = noisy.to(device, non_blocking=True), clean.to(device, non_blocking=True)\n",
        "\n",
        "        # For float 16\n",
        "        # noisy, clean = noisy.to(device, dtype=torch.float16), clean.to(device, dtype=torch.float16)\n",
        "\n",
        "        # noisy = force_even_dim(noisy)\n",
        "        # clean = force_even_dim(clean)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast(device_type=device_type):\n",
        "            output = model(noisy)\n",
        "\n",
        "\n",
        "            # Compute simplified losses\n",
        "            charbonnier = charbonnier_loss(output, clean)\n",
        "\n",
        "            # Ensure the tensor has enough pixels for gradient calculation\n",
        "            # if output.shape[-1] > 1 and output.shape[-2] > 1:\n",
        "            #     grad_loss = torch.mean(torch.abs(torch.gradient(output, dim=(-2, -1))[0] - torch.gradient(clean, dim=(-2, -1))[0]))\n",
        "            # else:\n",
        "            #     grad_loss = torch.tensor(0.0, device=device)  # Fallback if too small\n",
        "\n",
        "            laplacian = laplacian_loss(output)\n",
        "            tv = tv_loss(output)\n",
        "\n",
        "            loss = (\n",
        "                w1 * charbonnier +\n",
        "                # w2 * grad_loss + # not used\n",
        "                w3 * laplacian +\n",
        "                w4 * tv\n",
        "\n",
        "            )\n",
        "\n",
        "            loss = loss.mean()  # Ensure it's a scalar\n",
        "\n",
        "        # Backprop with scaled loss\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # GPU memory tracking\n",
        "        gpu_mem = torch.cuda.memory_allocated(device) / 1024**3\n",
        "\n",
        "        # Learning rate tracking\n",
        "        lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        # # Backpropagation\n",
        "        # loss.backward()\n",
        "        # optimizer.step()\n",
        "\n",
        "        # Track loss for logging\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        progress_bar.set_postfix({\n",
        "            \"charbonnier\": f\"{charbonnier.item():.6f}\",\n",
        "            # \"grad_loss\": f\"{grad_loss.item():.6f}\", # not used\n",
        "            \"lap_loss\": f\"{laplacian.item():.6f}\",\n",
        "            \"tv_loss\": f\"{tv.item():.6f}\",\n",
        "            \"loss\": f\"{loss.item():.4f}\",\n",
        "            \"gpu\": f\"{gpu_mem:.2f} GB\",\n",
        "            \"lr\": f\"{lr:.2e}\"\n",
        "        })\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for noisy, clean in val_loader:\n",
        "            noisy, clean = noisy.to(device), clean.to(device)\n",
        "            output = model(noisy)\n",
        "\n",
        "            charbonnier = charbonnier_loss(output, clean)\n",
        "            laplacian = laplacian_loss(output)\n",
        "            tv = tv_loss(output)\n",
        "            # ssim_score = ssim_torch(output, clean)\n",
        "            # ssim_loss = 1 - ssim_score\n",
        "            # loss = ssim_loss\n",
        "\n",
        "            loss = (\n",
        "                w1 * charbonnier +\n",
        "                w3 * laplacian +\n",
        "                w4 * tv\n",
        "            )\n",
        "            loss = loss.mean()  # or your total loss\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "    print(f\"Validation Loss: {val_loss:.6f}\")\n",
        "    train_losses.append(total_loss / len(train_loader))\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    # Use validation loss for learning rate scheduling\n",
        "    scheduler.step(val_loss)  # if you're using ReduceLROnPlateau\n",
        "\n",
        "    model.train()  # Switch back to training mode\n",
        "\n",
        "    # Early stop\n",
        "    # early_stopping(val_loss, model)\n",
        "\n",
        "    # Early stopping\n",
        "    # if early_stopping.early_stop:\n",
        "    #     print(\"Ran out of patience, early stopping!!\")\n",
        "    #     # Make sure folder exists (for extra safety)\n",
        "    #     os.makedirs(\"/content/model\", exist_ok=True)\n",
        "\n",
        "    #     print(\"Model already saved to /content/model/sar_durnet.pth\")\n",
        "    #     break\n",
        "    # Update LR Scheduler at the END of the epoch\n",
        "    # scheduler.step(total_loss / len(train_loader))\n",
        "\n",
        "\n",
        "\n",
        "# ========== 5. Save Model ==========\n",
        "os.makedirs(\"/content/model\", exist_ok=True)  # Make a folder exist\n",
        "torch.save(model.state_dict(), \"/content/model/SAR_DeepSpeck.pth\")\n",
        "print(\"Model saved!\")"
      ],
      "metadata": {
        "id": "6uVqV8g66UgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Took about 16GB of VRAM for 800 pairs, batch size 8, num worker = 4. \\\n",
        "\n",
        "25 second per epoch \\\n",
        "\n",
        "Took about 32GB of VRAM for 800 pairs, batch size 16, num worker = 8. \\\n",
        "\n",
        "25 second per epoch"
      ],
      "metadata": {
        "id": "XORqa7mzZ55P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try to plot the training loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_losses, label=\"Training Loss\", marker='o')\n",
        "plt.plot(val_losses, label=\"Validation Loss\", marker='s')\n",
        "plt.title(\"Training vs Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss (log scale)\")\n",
        "plt.yscale(\"log\")  # log scale\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dItv0ivNbpvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 5. Save Model (but manually) ==========\n",
        "# os.makedirs(\"/content/model\", exist_ok=True)  # Create folder if it doesn't exist\n",
        "# torch.save(model.state_dict(), \"/content/model/SAR_DeepSpeck.pth\")\n",
        "# print(\"Model saved!\")"
      ],
      "metadata": {
        "id": "y4XF5buUCR1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check what is eating up my gpu\n",
        "# !nvidia-smi\n"
      ],
      "metadata": {
        "id": "HaELMRSV6k3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load previous model (if any)"
      ],
      "metadata": {
        "id": "GJeIdeb2-n_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##########################\n",
        "# To load other models downloaded before\n",
        "#########################\n",
        "\n",
        "# Load the trained SAR-DURNet model\n",
        "model = SAR_DeepSpeck().to(device)\n",
        "\n",
        "# Load the saved model weights\n",
        "checkpoint_path = \"/content/model/SAR_DeepSpeck.pth\"\n",
        "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "print(\"Model successfully loaded!\")"
      ],
      "metadata": {
        "id": "O_WcZ9317pcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test on validation dataset"
      ],
      "metadata": {
        "id": "9m8t1pp5-hOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============= To test on validation images ==============================\n",
        "\n",
        "import glob\n",
        "import cv2\n",
        "import torch\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define paths\n",
        "noisy_folder = \"/content/test_data/SAR_Dataset/SAR despeckling filters dataset/Main folder/Noisy_val\"\n",
        "clean_folder = \"/content/test_data/SAR_Dataset/SAR despeckling filters dataset/Main folder/GTruth_val\"\n",
        "output_folder = \"/content/despeckled_val_results\"  # Where despeckled images will be saved\n",
        "\n",
        "# Make sure output folder exists\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Load model in eval mode\n",
        "model.eval()\n",
        "\n",
        "# Load noisy & clean image pairs\n",
        "noisy_images = sorted(glob.glob(noisy_folder + \"/*.tiff\"))\n",
        "clean_images = sorted(glob.glob(clean_folder + \"/*.tiff\"))\n",
        "\n",
        "assert len(noisy_images) == len(clean_images), \"Mismatch in number of noisy and clean images!\"\n",
        "\n",
        "# Store outputs for later visualization\n",
        "results = []\n",
        "\n",
        "# Resize to multiple of 32 for U-Net compatibility\n",
        "def resize_to_multiple_of_32(image):\n",
        "    h, w = image.shape\n",
        "    new_h = (h // 32) * 32\n",
        "    new_w = (w // 32) * 32\n",
        "    return cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "# Process each image\n",
        "for noisy_path, clean_path in zip(noisy_images, clean_images):\n",
        "    # print(f\"Processing {noisy_path}...\")\n",
        "\n",
        "    # Load images\n",
        "    noisy = cv2.imread(noisy_path, cv2.IMREAD_GRAYSCALE) / 255.0\n",
        "    clean = cv2.imread(clean_path, cv2.IMREAD_GRAYSCALE) / 255.0\n",
        "\n",
        "    # Resize\n",
        "    noisy_resized = resize_to_multiple_of_32(noisy)\n",
        "    clean_resized = resize_to_multiple_of_32(clean)\n",
        "\n",
        "    # Convert to tensor\n",
        "    noisy_tensor = torch.tensor(noisy_resized, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
        "\n",
        "    # Run through model\n",
        "    with torch.no_grad():\n",
        "        despeckled = model(noisy_tensor).squeeze().cpu().numpy()\n",
        "    despeckled = np.clip(despeckled, 0, 1)\n",
        "\n",
        "    # Save despeckled image\n",
        "    output_path = os.path.join(output_folder, os.path.basename(noisy_path))\n",
        "    cv2.imwrite(output_path, (despeckled * 255).astype(np.uint8))\n",
        "\n",
        "    # Store for visualization\n",
        "    results.append((noisy_resized, despeckled, clean_resized))\n",
        "\n",
        "print(\"ALL VALIDATION IMAGES PROCESSED!\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T-VaKbmdlj4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show 15 random samples at the end\n",
        "num_samples = min(15, len(results))\n",
        "sample_indices = np.random.choice(len(results), num_samples, replace=False)\n",
        "\n",
        "plt.figure(figsize=(5, 30))\n",
        "for i, idx in enumerate(sample_indices):\n",
        "    noisy, despeckled, clean = results[idx]\n",
        "\n",
        "    plt.subplot(num_samples, 3, i * 3 + 1)\n",
        "    plt.imshow(noisy, cmap=\"gray\")\n",
        "    plt.title(\"Noisy Input\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(num_samples, 3, i * 3 + 2)\n",
        "    plt.imshow(despeckled, cmap=\"gray\")\n",
        "    plt.title(\"Despeckled Output\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(num_samples, 3, i * 3 + 3)\n",
        "    plt.imshow(clean, cmap=\"gray\")\n",
        "    plt.title(\"Ground Truth\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pXxEbwaBmPai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# zip output folder to download ezpzggwp\n",
        "\n",
        "import shutil\n",
        "shutil.make_archive(\"/content/despeckled_val_results\", 'zip', \"/content/despeckled_val_results\")"
      ],
      "metadata": {
        "id": "i1PDD24FmgV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To use model on real SAR images"
      ],
      "metadata": {
        "id": "FX-Z0ElI-agj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use model on real SAR images"
      ],
      "metadata": {
        "id": "o5zP48trGeX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============== To test on real SAR images ================================\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def extract_overlapping_patches_with_padding(image, patch_size=256, stride=128):\n",
        "    h, w = image.shape\n",
        "\n",
        "    pad_h = (np.ceil((h - patch_size) / stride) * stride + patch_size - h).astype(int)\n",
        "    pad_w = (np.ceil((w - patch_size) / stride) * stride + patch_size - w).astype(int)\n",
        "\n",
        "    padded_image = np.pad(image, ((0, pad_h), (0, pad_w)), mode='reflect')\n",
        "    padded_h, padded_w = padded_image.shape\n",
        "\n",
        "    patches = []\n",
        "    positions = []\n",
        "\n",
        "    for y in range(0, padded_h - patch_size + 1, stride):\n",
        "        for x in range(0, padded_w - patch_size + 1, stride):\n",
        "            patch = padded_image[y:y+patch_size, x:x+patch_size]\n",
        "            patches.append(patch)\n",
        "            positions.append((y, x))\n",
        "\n",
        "    return patches, positions, padded_image.shape, (h, w)\n",
        "\n",
        "\n",
        "def merge_overlapping_patches(patches, positions, padded_shape, original_shape, patch_size=256):\n",
        "    h_pad, w_pad = padded_shape\n",
        "    h_orig, w_orig = original_shape\n",
        "\n",
        "    recon = np.zeros((h_pad, w_pad), dtype=np.float32)\n",
        "    weight = np.zeros((h_pad, w_pad), dtype=np.float32)\n",
        "\n",
        "    for patch, (y, x) in zip(patches, positions):\n",
        "        recon[y:y+patch_size, x:x+patch_size] += patch\n",
        "        weight[y:y+patch_size, x:x+patch_size] += 1.0\n",
        "\n",
        "    recon /= weight\n",
        "    return recon[:h_orig, :w_orig]  # Crop back to original\n",
        "\n",
        "def create_gaussian_weight(patch_size=256, sigma=0.125):\n",
        "    \"\"\"Create a smooth 2D Gaussian weight mask for blending overlapping patches.\"\"\"\n",
        "    ax = np.linspace(-1, 1, patch_size)\n",
        "    gauss = np.exp(-0.5 * (ax / sigma) ** 2)\n",
        "    weight = np.outer(gauss, gauss)\n",
        "    return weight / weight.max()\n",
        "\n",
        "\n",
        "def merge_patches_soft(patches, positions, padded_shape, original_shape, patch_size=256):\n",
        "    h_pad, w_pad = padded_shape\n",
        "    h_orig, w_orig = original_shape\n",
        "\n",
        "    recon = np.zeros((h_pad, w_pad), dtype=np.float32)\n",
        "    weight_sum = np.zeros((h_pad, w_pad), dtype=np.float32)\n",
        "\n",
        "    weight_mask = create_trimmed_weight(patch_size)  # Soft blending mask\n",
        "\n",
        "    for patch, (y, x) in zip(patches, positions):\n",
        "        weighted_patch = patch * weight_mask\n",
        "        recon[y:y+patch_size, x:x+patch_size] += weighted_patch\n",
        "        weight_sum[y:y+patch_size, x:x+patch_size] += weight_mask\n",
        "\n",
        "    final = recon / (weight_sum + 1e-8)\n",
        "    return final[:h_orig, :w_orig]  # Crop to original size\n",
        "\n",
        "def create_trimmed_weight(patch_size=256, inner_ratio=0.7, sigma=0.3):\n",
        "    ax = np.linspace(-1, 1, patch_size)\n",
        "    gauss = np.exp(-0.5 * (ax / sigma) ** 2)\n",
        "    outer = np.outer(gauss, gauss)\n",
        "    inner_mask = np.zeros_like(outer)\n",
        "\n",
        "    start = int(patch_size * ((1 - inner_ratio) / 2))\n",
        "    end = patch_size - start\n",
        "    inner_mask[start:end, start:end] = 1.0\n",
        "\n",
        "    trimmed = np.maximum(inner_mask, outer)\n",
        "    return trimmed / trimmed.max()\n",
        "\n",
        "\n",
        "# Process a single image through the model using patching\n",
        "def process_image_with_overlap_padded(image_path, model, device, patch_size=256, stride=128):\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE) / 255.0\n",
        "    image = image.astype(np.float32)\n",
        "\n",
        "    patches, positions, padded_shape, original_shape = extract_overlapping_patches_with_padding(\n",
        "        image, patch_size, stride)\n",
        "\n",
        "    output_patches = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for patch in patches:\n",
        "            tensor = torch.tensor(patch).unsqueeze(0).unsqueeze(0).to(device)\n",
        "            out = model(tensor).squeeze().cpu().numpy()\n",
        "            out = np.clip(out, 0, 1)\n",
        "            output_patches.append(out)\n",
        "\n",
        "    despeckled = merge_patches_soft(output_patches, positions, padded_shape, original_shape, patch_size)\n",
        "    return image, despeckled\n",
        "\n",
        "\n",
        "# ========== Run it on all images in your folder ==========\n",
        "\n",
        "input_folder = \"/content/imported_images/\"\n",
        "output_folder = \"/content/despeckled_results_patched/\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "image_paths = sorted([os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.lower().endswith((\".png\", \".jpg\", \".tif\", \".tiff\"))])\n",
        "\n",
        "for img_path in image_paths:\n",
        "    print(f\"Processing {img_path}\")\n",
        "    original, despeckled = process_image_with_overlap_padded(img_path, model, device)\n",
        "\n",
        "\n",
        "    # Save the result\n",
        "    out_name = os.path.basename(img_path)\n",
        "    output_path = os.path.join(output_folder, out_name)\n",
        "    cv2.imwrite(output_path, (despeckled * 255).astype(np.uint8))\n",
        "    print(f\"Saved to {output_path}\")\n",
        "\n",
        "    # Optional: Show comparison\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(original, cmap=\"gray\")\n",
        "    plt.title(\"Original SAR\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(despeckled, cmap=\"gray\")\n",
        "    plt.title(\"Despeckled\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "UEN-F3wT-Q-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# zip output folder to download ezpzggwp\n",
        "\n",
        "import shutil\n",
        "shutil.make_archive(\"/content/despeckled_results_patched\", 'zip', \"/content/despeckled_results_patched\")\n"
      ],
      "metadata": {
        "id": "qKx5tnHAxsrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "fH3ciPrmURcx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}